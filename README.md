# Loan Default Prediction System

A comprehensive machine learning system for predicting loan defaults using Medallion Architecture (Bronze-Silver-Gold) with advanced feature engineering, ensemble modeling, and a professional Streamlit web application.

## ğŸ¯ Project Overview

This system predicts loan defaults using multiple gradient boosting models (XGBoost, CatBoost, LightGBM) with advanced feature engineering to achieve high precision and recall on imbalanced data. The project follows a modular architecture with clear separation of concerns and follows the Medallion data architecture pattern.

## ğŸ“ Project Structure

```
PROJECT/
â”œâ”€â”€ data/                          # Data storage (Medallion Architecture)
â”‚   â”œâ”€â”€ raw/                       # Raw source data files
â”‚   â”œâ”€â”€ bronze/                    # Bronze Layer: Raw ingested data
â”‚   â”œâ”€â”€ silver/                    # Silver Layer: Cleaned, validated data
â”‚   â””â”€â”€ gold/                      # Gold Layer: Feature-engineered, modeling-ready
â”‚
â”œâ”€â”€ src/                           # Source code (Python modules)
â”‚   â”œâ”€â”€ data/                      # Data processing modules
â”‚   â”‚   â”œâ”€â”€ loaders.py             # Data loading functions
â”‚   â”‚   â”œâ”€â”€ cleaners.py            # Data cleaning functions
â”‚   â”‚   â””â”€â”€ transformers.py        # Data transformation functions
â”‚   â”‚
â”‚   â”œâ”€â”€ features/                  # Feature engineering
â”‚   â”‚   â””â”€â”€ interactions.py        # Interaction feature creation
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # Model-related code
â”‚   â”‚   â”œâ”€â”€ train.py               # Training functions
â”‚   â”‚   â”œâ”€â”€ evaluate.py            # Evaluation functions
â”‚   â”‚   â”œâ”€â”€ predict.py             # Prediction functions
â”‚   â”‚   â””â”€â”€ ensemble.py            # Ensemble methods
â”‚   â”‚
â”‚   â”œâ”€â”€ preprocessing/             # Preprocessing utilities
â”‚   â”‚   â”œâ”€â”€ encoders.py            # Label encoders
â”‚   â”‚   â””â”€â”€ samplers.py            # SMOTE, ADASYN, etc.
â”‚   â”‚
â”‚   â”œâ”€â”€ pipelines/                 # End-to-end pipelines
â”‚   â”‚   â”œâ”€â”€ bronze_pipeline.py     # Bronze layer processing
â”‚   â”‚   â”œâ”€â”€ silver_pipeline.py     # Silver layer processing
â”‚   â”‚   â”œâ”€â”€ gold_pipeline.py       # Gold layer processing
â”‚   â”‚   â”œâ”€â”€ training_pipeline.py   # Model training pipeline
â”‚   â”‚   â””â”€â”€ inference_pipeline.py  # Inference pipeline for new data
â”‚   â”‚
â”‚   â””â”€â”€ utils/                     # General utilities
â”‚       â”œâ”€â”€ paths.py               # Path management
â”‚       â””â”€â”€ helpers.py             # Helper functions
â”‚
â”œâ”€â”€ notebooks/                     # Jupyter notebooks for EDA
â”‚   â”œâ”€â”€ 01_data_exploration/       # Initial data exploration
â”‚   â”œâ”€â”€ 02_data_quality/           # Data quality analysis
â”‚   â”œâ”€â”€ 03_feature_analysis/       # Feature analysis
â”‚   â””â”€â”€ 04_model_analysis/         # Model analysis
â”‚
â”œâ”€â”€ scripts/                       # Executable scripts
â”‚   â”œâ”€â”€ run_bronze_pipeline.py     # Ingest raw data
â”‚   â”œâ”€â”€ run_silver_pipeline.py     # Clean and validate
â”‚   â”œâ”€â”€ run_gold_pipeline.py       # Feature engineering
â”‚   â”œâ”€â”€ train_model.py             # Train models
â”‚   â””â”€â”€ evaluate_new_data.py       # Evaluate new data (inference)
â”‚
â”œâ”€â”€ models/                        # Trained models and artifacts
â”‚   â”œâ”€â”€ checkpoints/               # Model checkpoints
â”‚   â”œâ”€â”€ artifacts/                 # Preprocessing artifacts
â”‚   â””â”€â”€ metadata/                  # Model metadata
â”‚
â”œâ”€â”€ outputs/                       # Generated outputs
â”‚   â”œâ”€â”€ reports/                   # Analysis reports
â”‚   â”œâ”€â”€ visualizations/            # Plots and charts
â”‚   â””â”€â”€ predictions/               # Prediction outputs
â”‚
â”œâ”€â”€ config/                        # Configuration files
â”‚   â”œâ”€â”€ data_config.yaml           # Data paths and settings
â”‚   â”œâ”€â”€ model_config.yaml          # Model hyperparameters
â”‚   â””â”€â”€ pipeline_config.yaml       # Pipeline settings
â”‚
â”œâ”€â”€ app/                           # Streamlit application
â”‚   â”œâ”€â”€ main.py                    # Streamlit app entry point
â”‚   â”œâ”€â”€ pages/                     # Multi-page app structure
â”‚   â””â”€â”€ components/                # Reusable components
â”‚
â””â”€â”€ requirements.txt               # Python dependencies
```

## ğŸ—ï¸ Architecture: Medallion Pattern

The project follows the **Medallion Architecture** pattern for data warehousing:

1. **Bronze Layer** (`data/bronze/`): Raw ingested data, exactly as received from source
2. **Silver Layer** (`data/silver/`): Cleaned, validated, and standardized data
3. **Gold Layer** (`data/gold/`): Feature-engineered, business-ready data for modeling

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd PROJECT
pip install -r requirements.txt
```

### 2. Data Processing Pipeline

#### Step 1: Bronze Layer - Ingest Raw Data

```bash
python scripts/run_bronze_pipeline.py --input ../dataset.duckdb
```

This loads raw data from the source DuckDB database into the bronze layer.

#### Step 2: Silver Layer - Clean and Validate

```bash
python scripts/run_silver_pipeline.py --input data/raw/
```

This cleans, standardizes, and validates the data.

#### Step 3: Gold Layer - Feature Engineering

```bash
python scripts/run_gold_pipeline.py --input data/silver/cleaned_dataset.duckdb
```

This creates advanced features and prepares data for modeling.

### 3. Train Models

```bash
python scripts/train_model.py
```

This will:
- Load data from gold layer
- Train XGBoost, CatBoost, and LightGBM models
- Find optimal classification threshold
- Evaluate models and create ensemble
- Save models and artifacts

### 4. Evaluate New Data (Inference)

For new evaluation data (same structure but without `default` column):

```bash
python scripts/evaluate_new_data.py --input path/to/new_data_raw
```

This will:
- Process new data through Bronze â†’ Silver â†’ Gold layers
- Apply same preprocessing as training
- Make predictions using trained models
- Save predictions with probabilities and risk levels

### 5. Launch Streamlit App

```bash
streamlit run app/main.py
```

## ğŸ“Š Key Features

### Advanced Feature Engineering

- **Temporal Features**: Business hours, weekends, account age
- **Credit Risk Indicators**: Credit score categories, utilization ratios, delinquency flags
- **Financial Health Metrics**: DTI ratios, cash flow indicators, debt service capacity
- **Interaction Features**: Credit-DTI interactions, income-loan ratios, payment coverage
- **Categorical Standardization**: Normalized employment types, loan types, education levels

### Model Training

- **Ensemble Approach**: Combines XGBoost, CatBoost, and LightGBM
- **Class Imbalance Handling**: Optimized scale_pos_weight and class weights
- **Threshold Optimization**: Finds optimal threshold for balanced precision/recall
- **Early Stopping**: Prevents overfitting with validation monitoring

### Inference Pipeline

- **Complete Pipeline**: Processes new data through all layers
- **Consistent Preprocessing**: Uses same transformations as training
- **Handles Missing Target**: Automatically handles data without `default` column
- **Batch Processing**: Efficient processing of multiple records

## ğŸ”§ Usage Examples

### Using Modules in Python

```python
from src.data.loaders import load_data_from_duckdb
from src.data.cleaners import clean_application_metadata
from src.features.interactions import create_interaction_features
from src.models.train import train_xgboost_model
from src.models.predict import predict_batch

# Load data
dfs = load_data_from_duckdb('data/bronze/dataset.duckdb')

# Clean data
df_clean = clean_application_metadata(dfs['application_metadata'])

# Create features
df_features = create_interaction_features(df_clean)

# Train model
model = train_xgboost_model(X_train, y_train, X_val, y_val)

# Make predictions
predictions = predict_batch(X_new, models_dir='models/checkpoints')
```

### Running Complete Pipeline

```python
from src.pipelines.inference_pipeline import run_inference_pipeline

# Process new evaluation data
results = run_inference_pipeline(
    raw_data_path='data/raw/new_applications.duckdb',
    models_dir='models/checkpoints',
    artifacts_dir='models/artifacts',
    output_path='outputs/predictions/predictions.csv'
)
```

## ğŸ“ˆ Model Artifacts

After training, the following files are created:

- `models/checkpoints/xgb_model.json`: XGBoost model
- `models/checkpoints/catboost_model.cbm`: CatBoost model
- `models/checkpoints/lightgbm_model.txt`: LightGBM model
- `models/artifacts/label_encoders.pkl`: Categorical encoders
- `models/artifacts/optimal_threshold.pkl`: Optimal classification threshold
- `models/artifacts/feature_names.pkl`: Feature names (order matters!)
- `models/metadata/feature_importance.csv`: Feature importance scores

## ğŸ¨ Streamlit Application

The Streamlit app provides:
- **Single Prediction**: Interactive form for loan application details
- **Batch Prediction**: Upload CSV files for batch processing
- **Model Performance Dashboard**: View metrics and visualizations
- **Real-time Risk Assessment**: Get instant predictions with probabilities

## ğŸ“ Data Requirements

The system expects the following tables in DuckDB:

1. **application_metadata**: Application details, customer engagement
2. **credit_history**: Credit scores, accounts, payment history
3. **demographics**: Customer demographics, employment
4. **financial_ratios**: Income, debt, cash flow metrics
5. **geographic_data**: Regional economic indicators
6. **loan_details**: Loan characteristics, terms, purpose

## ğŸ” Inference Pipeline for New Data

The inference pipeline handles new evaluation data that:
- Has the same structure as training data
- **Does NOT have the `default` column** (target variable)
- Needs to be processed through the same pipeline

The pipeline automatically:
1. Processes data through Bronze â†’ Silver â†’ Gold layers
2. Applies same cleaning and feature engineering
3. Uses saved preprocessing artifacts (encoders, feature names)
4. Makes predictions using trained models
5. Outputs probabilities, predictions, and risk levels

## ğŸ“š Technical Details

### Class Imbalance Handling

The dataset has ~5% default rate. The system handles this through:
- Weighted loss functions
- Optimized class weights
- SMOTE/ADASYN oversampling
- Threshold tuning
- Ensemble averaging

### Feature Engineering Strategy

1. **Cleaning**: Handle missing values, standardize formats
2. **Encoding**: Label encoding for categoricals
3. **Binning**: Create categorical features from continuous
4. **Interactions**: Multiply and divide related features
5. **Risk Indicators**: Binary flags for high-risk conditions

### Model Selection

The ensemble combines:
- **XGBoost**: Best for structured data, handles missing values
- **CatBoost**: Excellent for categorical features
- **LightGBM**: Fast training, good performance

Final prediction is the average of all three models.

## ğŸ¤ Contributing

To improve the model:
1. Experiment with additional features in `src/features/`
2. Try different ensemble weights
3. Tune hyperparameters in `config/model_config.yaml`
4. Add more models to the ensemble

## ğŸ“„ License

This project is developed for bank loan risk assessment.

## ğŸ™ Acknowledgments

Built with:
- XGBoost, CatBoost, LightGBM
- Streamlit for web interface
- DuckDB for data management
- Plotly for visualizations

---

**For questions or issues, please refer to the code comments or contact the development team.**

